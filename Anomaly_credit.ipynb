{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7aPuULNKNKORRM1w1j2hz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kUggTTZuqb-h"},"outputs":[],"source":["'''Use Autoencoder to implement anomaly detection. Build the model by using:\n","a. Import required libraries\n","b. Upload/access the dataset\n","c. An encoder converts it into latent representation\n","d. Decoder networks convert it back to the original input\n","e. Compile the models with Optimizer, Loss, and Evaluation Metrics\n","Dataset: Credit Card'''\n","\n","# a. IMPORTING REQUIRED LIBRARIES ->\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score\n","\n","RANDOM_SEED = 2021\n","TEST_PCT = 0.3\n","LABELS = [\"Normal\", \"Fraud\"]\n","\n","# b. UPLOAD / ACCESS THE DATASET ->\n","dataset = pd.read_csv(\"/content/creditcard.csv\")\n","\n","# Check for any null values\n","print(\"Any nulls in the dataset\", dataset.isnull().values.any())\n","print('-------')\n","print(\"No. of unique labels\", len(dataset['Class'].unique()))\n","print(\"Label values\", dataset.Class.unique())\n","\n","# 0 is for normal credit card transaction\n","# 1 is for fraudulent credit card transaction\n","print('-------')\n","print(\"Break down of Normal and Fraud Transactions\")\n","print(pd.value_counts(dataset['Class'], sort=True))\n","\n","# Visualizing the imbalanced dataset\n","count_classes = pd.value_counts(dataset['Class'], sort=True)\n","count_classes.plot(kind='bar', rot=0)\n","plt.xticks(range(len(dataset['Class'].unique())), dataset.Class.unique())\n","plt.title(\"Frequency by observation number\")\n","plt.xlabel(\"Class\")\n","plt.ylabel(\"Number of Observations\")\n","\n","# Save the normal and fraudulent transactions in separate dataframes\n","normal_dataset = dataset[dataset.Class == 0]\n","fraud_dataset = dataset[dataset.Class == 1]\n","\n","# Visualize transaction amounts for normal and fraudulent transactions\n","bins = np.linspace(200, 2500, 100)\n","plt.hist(normal_dataset.Amount, bins=bins, alpha=1, density=True, label='Normal')\n","plt.hist(fraud_dataset.Amount, bins=bins, alpha=0.5, density=True, label='Fraud')\n","plt.legend(loc='upper right')\n","plt.title(\"Transaction Amount vs Percentage of Transactions\")\n","plt.xlabel(\"Transaction Amount (USD)\")\n","plt.ylabel(\"Percentage of Transactions\")\n","plt.show()\n","\n","sc = StandardScaler()\n","dataset['Time'] = sc.fit_transform(dataset['Time'].values.reshape(-1, 1))\n","dataset['Amount'] = sc.fit_transform(dataset['Amount'].values.reshape(-1, 1))\n","\n","raw_data = dataset.values\n","# The last element contains if the transaction is normal (0) or fraud (1)\n","labels = raw_data[:, -1]\n","\n","# The other data points are the electrocardiogram data\n","data = raw_data[:, 0:-1]\n","\n","train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=2021)\n","\n","min_val = tf.reduce_min(train_data)\n","max_val = tf.reduce_max(train_data)\n","\n","train_data = (train_data - min_val) / (max_val - min_val)\n","test_data = (test_data - min_val) / (max_val - min_val)\n","\n","train_data = tf.cast(train_data, tf.float32)\n","test_data = tf.cast(test_data, tf.float32)\n","\n","train_labels = train_labels.astype(bool)\n","test_labels = test_labels.astype(bool)\n","\n","# Creating normal and fraud datasets\n","normal_train_data = train_data[~train_labels]\n","normal_test_data = test_data[~test_labels]\n","\n","fraud_train_data = train_data[train_labels]\n","fraud_test_data = test_data[test_labels]\n","print(\"No. of records in Fraud Train Data =\", len(fraud_train_data))\n","print(\"No. of records in Normal Train Data =\", len(normal_train_data))\n","print(\"No. of records in Fraud Test Data =\", len(fraud_test_data))\n","print(\"No. of records in Normal Test Data =\", len(normal_test_data))\n","\n","nb_epoch = 50\n","batch_size = 64\n","input_dim = normal_train_data.shape[1]\n","# num of columns, 30\n","encoding_dim = 14\n","hidden_dim1 = int(encoding_dim / 2)\n","hidden_dim2 = 4\n","learning_rate = 1e-7\n","\n","# input layer\n","input_layer = tf.keras.layers.Input(shape=(input_dim,))\n","\n","# c. ENCODER CONVERTS IT INTO LATENT REPRESENTATION\n","\n","# Encoder\n","encoder = tf.keras.layers.Dense(encoding_dim, activation=\"tanh\", activity_regularizer=tf.keras.regularizers.l2(learning_rate))(input_layer)\n","encoder = tf.keras.layers.Dropout(0.2)(encoder)\n","encoder = tf.keras.layers.Dense(hidden_dim1, activation='relu')(encoder)\n","encoder = tf.keras.layers.Dense(hidden_dim2, activation=tf.nn.leaky_relu)(encoder)\n","\n","# d. DECODER NETWORKS CONVERT IT BACK TO THE ORIGINAL INPUT ->\n","# Decoder\n","decoder = tf.keras.layers.Dense(hidden_dim1, activation='relu')(encoder)\n","decoder = tf.keras.layers.Dropout(0.2)(decoder)\n","decoder = tf.keras.layers.Dense(encoding_dim, activation='relu')(decoder)\n","decoder = tf.keras.layers.Dense(input_dim, activation='tanh')(decoder)\n","\n","# Autoencoder\n","autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)\n","autoencoder.summary()\n","\n","cp = tf.keras.callbacks.ModelCheckpoint(filepath=\"autoencoder_fraud.h5\", mode='min', monitor='val_loss', verbose=2, save_best_only=True)\n","# Define our early stopping\n","early_stop = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    min_delta=0.0001,\n","    patience=10,\n","    verbose=11,\n","    mode='min',\n","    restore_best_weights=True\n",")\n","\n","# e. COMPILE THE MODEL WITH OPTIMIZER, LOSS, AND EVALUATION METRICS ->\n","autoencoder.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n","\n","history = autoencoder.fit(normal_train_data, normal_train_data, epochs=nb_epoch,\n","                          batch_size=batch_size, shuffle=True,\n","                          validation_data=(test_data, test_data),\n","                          verbose=1,\n","                          callbacks=[cp, early_stop]).history\n","\n","plt.plot(history['loss'], linewidth=2, label='Train')\n","plt.plot(history['val_loss'], linewidth=2, label='Test')\n","plt.legend(loc='upper right')\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.show()\n","\n","test_x_predictions = autoencoder.predict(test_data)\n","mse = np.mean(np.power(test_data - test_x_predictions, 2), axis=1)\n","error_df = pd.DataFrame({'Reconstruction_error': mse,\n","                         'True_class': test_labels})\n","\n","threshold_fixed = 50\n","groups = error_df.groupby('True_class')\n","fig, ax = plt.subplots()\n","\n","for name, group in groups:\n","    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n","            label=\"Fraud\" if name == 1 else \"Normal\")\n","ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label=\"Threshold\")\n","ax.legend()\n","plt.title(\"Reconstruction error for normal and fraud data\")\n","plt.ylabel(\"Reconstruction error\")\n","plt.xlabel(\"Data point index\")\n","plt.show()\n","\n","threshold_fixed = 52\n","pred_y = [1 if e > threshold_fixed else 0\n","          for e in\n","          error_df.Reconstruction_error.values]\n","error_df['pred'] = pred_y\n","conf_matrix = confusion_matrix(error_df.True_class, pred_y)\n","\n","plt.figure(figsize=(4, 4))\n","sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\")\n","plt.title(\"Confusion matrix\")\n","plt.ylabel(\"True class\")\n","plt.xlabel(\"Predicted class\")\n","plt.show()\n","\n","# Print Accuracy, Precision, and Recall\n","print(\"Accuracy:\", accuracy_score(error_df['True_class'], error_df['pred']))\n","print(\"Recall:\", recall_score(error_df['True_class'], error_df['pred']))\n","print(\"Precision:\", precision_score(error_df['True_class'], error_df['pred']))"]}]}